{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "## Check for CUDA GPU (skip if no CUDA GPU) ##\n",
    "\n",
    "print(tf.test.is_built_with_cuda()) # Should be True\n",
    "print(tf.config.list_physical_devices('GPU')) # Should not be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEEP LEARNING ###\n",
    "from helper import *\n",
    "\n",
    "## EXTRACTING X AND Y MATRICES. READ HELPER FOR FUNCTION INFO ##\n",
    "dh = data_handle()\n",
    "\n",
    "classes = (2,5,11)\n",
    "img_size = 128\n",
    "\n",
    "X, y = dh.featurize(resize=img_size, n=750, include=classes)\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792, 16384) (449, 16384)\n"
     ]
    }
   ],
   "source": [
    "## Preprocess images ##\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "range_ = np.array(list(set(y)))\n",
    "out = lb.fit(range_)\n",
    "y_binary = out.transform(y.astype(int))\n",
    "\n",
    "scalify = preprocessing.StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train = scalify.fit_transform(X_train)\n",
    "X_test = scalify.fit_transform(X_test)\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X, y_binary, test_size=0.20, random_state=42)\n",
    "X_train_b = scalify.fit_transform(X_train_b)\n",
    "X_test_b = scalify.fit_transform(X_test_b)\n",
    "\n",
    "print(X_train_b.shape, X_test_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AlexNet variant with adjusted image size ##\n",
    "\n",
    "alexnet = Sequential([\n",
    "    Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(img_size,img_size,1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 30, 30, 96)        11712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 30, 30, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 256)       614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 6, 384)         147840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 6, 6, 256)         98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 4096)              4198400   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 12291     \n",
      "=================================================================\n",
      "Total params: 22,755,395\n",
      "Trainable params: 22,752,643\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Compile and display full network ##\n",
    "\n",
    "alexnet.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "alexnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes a bigass file for saving weights, be aware ##\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_weights_alexnet.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 2/50 [>.............................] - ETA: 1s - loss: 16.2668 - accuracy: 0.3281WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.0539s). Check your callbacks.\n",
      "50/50 [==============================] - 4s 74ms/step - loss: 2.6496 - accuracy: 0.3341 - val_loss: 1.0999 - val_accuracy: 0.3519\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 3s 68ms/step - loss: 1.1080 - accuracy: 0.3663 - val_loss: 1.0839 - val_accuracy: 0.3831\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 1.0948 - accuracy: 0.4016 - val_loss: 1.1503 - val_accuracy: 0.3541\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 1.0810 - accuracy: 0.4166 - val_loss: 1.0997 - val_accuracy: 0.3408\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 1.0156 - accuracy: 0.4762 - val_loss: 1.0876 - val_accuracy: 0.3697\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 3s 56ms/step - loss: 0.9825 - accuracy: 0.5094 - val_loss: 1.0987 - val_accuracy: 0.3608\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 3s 57ms/step - loss: 0.9593 - accuracy: 0.5272 - val_loss: 1.0940 - val_accuracy: 0.3764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19919bde588>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.fit(X_train_b.reshape(-1,img_size,img_size,1), y_train_b, steps_per_epoch=50,\n",
    "           epochs=20, batch_size=64, validation_data=(X_test_b.reshape(-1,img_size,img_size,1), y_test_b), \n",
    "           callbacks=[checkpoint, lr_reduce, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.30888242 0.34784117 0.3432764 ], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Check how confident prediction on random image is ##\n",
    "\n",
    "predictions = alexnet.predict(X_test.reshape(-1,img_size,img_size,1))\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 11ms/step - loss: 1.0839 - accuracy: 0.3831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0839285850524902, 0.38307350873947144]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.evaluate(X_test.reshape(-1,img_size,img_size,1), y_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VGG variant with adjusted image size ##\n",
    "\n",
    "vgg = Sequential([\n",
    "    Conv2D(16, 3, activation='relu', padding='same', input_shape=(img_size,img_size,1)),\n",
    "    MaxPool2D(pool_size=(2,2)),\n",
    "    SeparableConv2D(32, 3, activation='relu', padding='same'),\n",
    "    SeparableConv2D(32, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(),\n",
    "    SeparableConv2D(64, 3, activation='relu', padding='same'),\n",
    "    SeparableConv2D(64, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(),\n",
    "    SeparableConv2D(128, 3, activation='relu', padding='same'),\n",
    "    SeparableConv2D(128, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(),\n",
    "    SeparableConv2D(256, 3, activation='relu', padding='same'),\n",
    "    SeparableConv2D(256, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(),\n",
    "    SeparableConv2D(512, 3, activation='relu', padding='same'),\n",
    "    SeparableConv2D(512, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(),\n",
    "    Flatten(),\n",
    "    Dense(1028, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.9),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 128, 128, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_20 (Separab (None, 64, 64, 32)        688       \n",
      "_________________________________________________________________\n",
      "separable_conv2d_21 (Separab (None, 64, 64, 32)        1344      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_22 (Separab (None, 32, 32, 64)        2400      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_23 (Separab (None, 32, 32, 64)        4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_24 (Separab (None, 16, 16, 128)       8896      \n",
      "_________________________________________________________________\n",
      "separable_conv2d_25 (Separab (None, 16, 16, 128)       17664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_26 (Separab (None, 8, 8, 256)         34176     \n",
      "_________________________________________________________________\n",
      "separable_conv2d_27 (Separab (None, 8, 8, 256)         68096     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_28 (Separab (None, 4, 4, 512)         133888    \n",
      "_________________________________________________________________\n",
      "separable_conv2d_29 (Separab (None, 4, 4, 512)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1028)              2106372   \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 1028)              4112      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1028)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 512)               526848    \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,257,543\n",
      "Trainable params: 3,252,095\n",
      "Non-trainable params: 5,448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes a bigass file for saving weights, be aware ##\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_weights_vgg_like.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 2/50 [>.............................] - ETA: 3s - loss: 1.4302 - accuracy: 0.4219WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.1277s). Check your callbacks.\n",
      "50/50 [==============================] - 7s 149ms/step - loss: 1.6195 - accuracy: 0.3487 - val_loss: 1.1046 - val_accuracy: 0.3163\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 1.4503 - accuracy: 0.3325 - val_loss: 1.1126 - val_accuracy: 0.3163\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 1.3304 - accuracy: 0.3378 - val_loss: 1.1166 - val_accuracy: 0.3163\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 1.3009 - accuracy: 0.3269 - val_loss: 1.1166 - val_accuracy: 0.3163\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 7s 136ms/step - loss: 1.2924 - accuracy: 0.3250 - val_loss: 1.1168 - val_accuracy: 0.3163\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 7s 137ms/step - loss: 1.2900 - accuracy: 0.3391 - val_loss: 1.1181 - val_accuracy: 0.3163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199148e15c0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.fit(X_train_b.reshape(-1,img_size,img_size,1), y_train_b, steps_per_epoch=50,\n",
    "           epochs=20, batch_size=64, validation_data=(X_test_b.reshape(-1,img_size,img_size,1), y_test_b), \n",
    "           callbacks=[checkpoint, lr_reduce, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 17ms/step - loss: 1.1046 - accuracy: 0.3163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1045939922332764, 0.31625834107398987]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.evaluate(X_test_b.reshape(-1,img_size,img_size,1), y_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b_rgb = np.zeros((len(X_train_b), img_size, img_size, 3))\n",
    "X_test_b_rgb = np.zeros((len(X_test_b), img_size, img_size, 3))\n",
    "\n",
    "for i in range(len(X_train_b)):\n",
    "    X_train_b_rgb[i] = cv2.cvtColor(X_train_b[i],cv2.COLOR_GRAY2RGB).reshape(-1, img_size, img_size, 3)\n",
    "for i in range(len(X_test_b)):\n",
    "    X_test_b_rgb[i] = cv2.cvtColor(X_test_b[i],cv2.COLOR_GRAY2RGB).reshape(-1, img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-103.25762265, -116.09762265, -122.99862265],\n",
       "         [-103.35187851, -116.19187851, -123.09287851],\n",
       "         [-103.50867387, -116.34867387, -123.24967387],\n",
       "         ...,\n",
       "         [-103.73514629, -116.57514629, -123.47614629],\n",
       "         [-103.8273211 , -116.6673211 , -123.5683211 ],\n",
       "         [-103.85495468, -116.69495468, -123.59595468]],\n",
       "\n",
       "        [[-103.05520605, -115.89520605, -122.79620605],\n",
       "         [-103.23162853, -116.07162853, -122.97262853],\n",
       "         [-103.34960591, -116.18960591, -123.09060591],\n",
       "         ...,\n",
       "         [-103.54413088, -116.38413088, -123.28513088],\n",
       "         [-103.54238575, -116.38238575, -123.28338575],\n",
       "         [-103.65389891, -116.49389891, -123.39489891]],\n",
       "\n",
       "        [[-103.03854494, -115.87854494, -122.77954494],\n",
       "         [-103.07373284, -115.91373284, -122.81473284],\n",
       "         [-103.25773316, -116.09773316, -122.99873316],\n",
       "         ...,\n",
       "         [-103.79431469, -116.63431469, -123.53531469],\n",
       "         [-103.94573345, -116.78573345, -123.68673345],\n",
       "         [-103.99178995, -116.83178995, -123.73278995]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-105.36121332, -118.20121332, -125.10221332],\n",
       "         [-105.4866079 , -118.3266079 , -125.2276079 ],\n",
       "         [-105.52590476, -118.36590476, -125.26690476],\n",
       "         ...,\n",
       "         [-105.33728897, -118.17728897, -125.07828897],\n",
       "         [-105.25370144, -118.09370144, -124.99470144],\n",
       "         [-105.1832224 , -118.0232224 , -124.9242224 ]],\n",
       "\n",
       "        [[-105.35749625, -118.19749625, -125.09849625],\n",
       "         [-105.48316442, -118.32316442, -125.22416442],\n",
       "         [-105.5213971 , -118.3613971 , -125.2623971 ],\n",
       "         ...,\n",
       "         [-105.33579503, -118.17579503, -125.07679503],\n",
       "         [-105.25846933, -118.09846933, -124.99946933],\n",
       "         [-105.18921029, -118.02921029, -124.93021029]],\n",
       "\n",
       "        [[-105.35410797, -118.19410797, -125.09510797],\n",
       "         [-105.50075244, -118.34075244, -125.24175244],\n",
       "         [-105.51313685, -118.35313685, -125.25413685],\n",
       "         ...,\n",
       "         [-105.33806549, -118.17806549, -125.07906549],\n",
       "         [-105.25476514, -118.09476514, -124.99576514],\n",
       "         [-105.18305384, -118.02305384, -124.92405384]]],\n",
       "\n",
       "\n",
       "       [[[-103.94485072, -116.78485072, -123.68585072],\n",
       "         [-103.69082308, -116.53082308, -123.43182308],\n",
       "         [-103.61880582, -116.45880582, -123.35980582],\n",
       "         ...,\n",
       "         [-104.15862111, -116.99862111, -123.89962111],\n",
       "         [-104.07840454, -116.91840454, -123.81940454],\n",
       "         [-104.18350257, -117.02350257, -123.92450257]],\n",
       "\n",
       "        [[-103.71113939, -116.55113939, -123.45213939],\n",
       "         [-103.37605648, -116.21605648, -123.11705648],\n",
       "         [-103.59203865, -116.43203865, -123.33303865],\n",
       "         ...,\n",
       "         [-104.11096408, -116.95096408, -123.85196408],\n",
       "         [-104.27429708, -117.11429708, -124.01529708],\n",
       "         [-104.27368992, -117.11368992, -124.01468992]],\n",
       "\n",
       "        [[-103.44175365, -116.28175365, -123.18275365],\n",
       "         [-103.56726812, -116.40726812, -123.30826812],\n",
       "         [-103.73294324, -116.57294324, -123.47394324],\n",
       "         ...,\n",
       "         [-104.23605682, -117.07605682, -123.97705682],\n",
       "         [-104.23465448, -117.07465448, -123.97565448],\n",
       "         [-104.27161859, -117.11161859, -124.01261859]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-105.33894013, -118.17894013, -125.07994013],\n",
       "         [-105.51387547, -118.35387547, -125.25487547],\n",
       "         [-104.98972439, -117.82972439, -124.73072439],\n",
       "         ...,\n",
       "         [-105.43264674, -118.27264674, -125.17364674],\n",
       "         [-105.54955577, -118.38955577, -125.29055577],\n",
       "         [-103.30874943, -116.14874943, -123.04974943]],\n",
       "\n",
       "        [[-105.3186134 , -118.1586134 , -125.0596134 ],\n",
       "         [-105.51203178, -118.35203178, -125.25303178],\n",
       "         [-104.97525381, -117.81525381, -124.71625381],\n",
       "         ...,\n",
       "         [-105.41796004, -118.25796004, -125.15896004],\n",
       "         [-105.55048047, -118.39048047, -125.29148047],\n",
       "         [-104.13494678, -116.97494678, -123.87594678]],\n",
       "\n",
       "        [[-105.29703831, -118.13703831, -125.03803831],\n",
       "         [-105.49508165, -118.33508165, -125.23608165],\n",
       "         [-104.99493491, -117.83493491, -124.73593491],\n",
       "         ...,\n",
       "         [-105.43123042, -118.27123042, -125.17223042],\n",
       "         [-105.52394449, -118.36394449, -125.26494449],\n",
       "         [-104.3701032 , -117.2101032 , -124.1111032 ]]],\n",
       "\n",
       "\n",
       "       [[[-103.92993636, -116.76993636, -123.67093636],\n",
       "         [-104.0861148 , -116.9261148 , -123.8271148 ],\n",
       "         [-104.10476345, -116.94476345, -123.84576345],\n",
       "         ...,\n",
       "         [-103.86546762, -116.70546762, -123.60646762],\n",
       "         [-103.87398981, -116.71398981, -123.61498981],\n",
       "         [-104.01761015, -116.85761015, -123.75861015]],\n",
       "\n",
       "        [[-104.18547513, -117.02547513, -123.92647513],\n",
       "         [-104.02469033, -116.86469033, -123.76569033],\n",
       "         [-104.13416487, -116.97416487, -123.87516487],\n",
       "         ...,\n",
       "         [-103.94110888, -116.78110888, -123.68210888],\n",
       "         [-104.009731  , -116.849731  , -123.750731  ],\n",
       "         [-104.09629383, -116.93629383, -123.83729383]],\n",
       "\n",
       "        [[-104.63944512, -117.47944512, -124.38044512],\n",
       "         [-104.19178795, -117.03178795, -123.93278795],\n",
       "         [-104.08353131, -116.92353131, -123.82453131],\n",
       "         ...,\n",
       "         [-104.02181172, -116.86181172, -123.76281172],\n",
       "         [-104.04733214, -116.88733214, -123.78833214],\n",
       "         [-104.16184992, -117.00184992, -123.90284992]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-103.32692044, -116.16692044, -123.06792044],\n",
       "         [-103.37206464, -116.21206464, -123.11306464],\n",
       "         [-103.38764899, -116.22764899, -123.12864899],\n",
       "         ...,\n",
       "         [-103.49440815, -116.33440815, -123.23540815],\n",
       "         [-103.23056572, -116.07056572, -122.97156572],\n",
       "         [-103.34352957, -116.18352957, -123.08452957]],\n",
       "\n",
       "        [[-103.3096674 , -116.1496674 , -123.0506674 ],\n",
       "         [-103.35762631, -116.19762631, -123.09862631],\n",
       "         [-103.43063979, -116.27063979, -123.17163979],\n",
       "         ...,\n",
       "         [-103.48498314, -116.32498314, -123.22598314],\n",
       "         [-103.23816823, -116.07816823, -122.97916823],\n",
       "         [-103.33584355, -116.17584355, -123.07684355]],\n",
       "\n",
       "        [[-103.30999642, -116.14999642, -123.05099642],\n",
       "         [-103.32435887, -116.16435887, -123.06535887],\n",
       "         [-103.46608399, -116.30608399, -123.20708399],\n",
       "         ...,\n",
       "         [-103.48454915, -116.32454915, -123.22554915],\n",
       "         [-103.24750834, -116.08750834, -122.98850834],\n",
       "         [-103.33514654, -116.17514654, -123.07614654]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-102.5420283 , -115.3820283 , -122.2830283 ],\n",
       "         [-102.62339209, -115.46339209, -122.36439209],\n",
       "         [-102.65432476, -115.49432476, -122.39532476],\n",
       "         ...,\n",
       "         [-102.43471204, -115.27471204, -122.17571204],\n",
       "         [-102.58118354, -115.42118354, -122.32218354],\n",
       "         [-102.6524372 , -115.4924372 , -122.3934372 ]],\n",
       "\n",
       "        [[-102.63540851, -115.47540851, -122.37640851],\n",
       "         [-102.6306585 , -115.4706585 , -122.3716585 ],\n",
       "         [-102.65734803, -115.49734803, -122.39834803],\n",
       "         ...,\n",
       "         [-102.58198797, -115.42198797, -122.32298797],\n",
       "         [-102.63872432, -115.47872432, -122.37972432],\n",
       "         [-102.66480671, -115.50480671, -122.40580671]],\n",
       "\n",
       "        [[-102.69784938, -115.53784938, -122.43884938],\n",
       "         [-102.6898719 , -115.5298719 , -122.4308719 ],\n",
       "         [-102.68884372, -115.52884372, -122.42984372],\n",
       "         ...,\n",
       "         [-102.53921508, -115.37921508, -122.28021508],\n",
       "         [-102.59902321, -115.43902321, -122.34002321],\n",
       "         [-102.50694107, -115.34694107, -122.24794107]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-105.31921898, -118.15921898, -125.06021898],\n",
       "         [-105.47167002, -118.31167002, -125.21267002],\n",
       "         [-105.28307914, -118.12307914, -125.02407914],\n",
       "         ...,\n",
       "         [-103.12863653, -115.96863653, -122.86963653],\n",
       "         [-103.10528999, -115.94528999, -122.84628999],\n",
       "         [-103.07924538, -115.91924538, -122.82024538]],\n",
       "\n",
       "        [[-105.3778634 , -118.2178634 , -125.1188634 ],\n",
       "         [-105.4322915 , -118.2722915 , -125.1732915 ],\n",
       "         [-105.56734787, -118.40734787, -125.30834787],\n",
       "         ...,\n",
       "         [-103.15772395, -115.99772395, -122.89872395],\n",
       "         [-103.12598072, -115.96598072, -122.86698072],\n",
       "         [-103.06686603, -115.90686603, -122.80786603]],\n",
       "\n",
       "        [[-105.34717642, -118.18717642, -125.08817642],\n",
       "         [-105.43388473, -118.27388473, -125.17488473],\n",
       "         [-105.53674566, -118.37674566, -125.27774566],\n",
       "         ...,\n",
       "         [-103.14356553, -115.98356553, -122.88456553],\n",
       "         [-103.11968844, -115.95968844, -122.86068844],\n",
       "         [-103.05046468, -115.89046468, -122.79146468]]],\n",
       "\n",
       "\n",
       "       [[[-104.92217671, -117.76217671, -124.66317671],\n",
       "         [-104.92959516, -117.76959516, -124.67059516],\n",
       "         [-104.93818753, -117.77818753, -124.67918753],\n",
       "         ...,\n",
       "         [-105.02807807, -117.86807807, -124.76907807],\n",
       "         [-105.01314424, -117.85314424, -124.75414424],\n",
       "         [-105.00514196, -117.84514196, -124.74614196]],\n",
       "\n",
       "        [[-105.02359604, -117.86359604, -124.76459604],\n",
       "         [-105.03338944, -117.87338944, -124.77438944],\n",
       "         [-104.993932  , -117.833932  , -124.734932  ],\n",
       "         ...,\n",
       "         [-105.10869073, -117.94869073, -124.84969073],\n",
       "         [-105.1314144 , -117.9714144 , -124.8724144 ],\n",
       "         [-104.90881126, -117.74881126, -124.64981126]],\n",
       "\n",
       "        [[-104.89517998, -117.73517998, -124.63617998],\n",
       "         [-104.8941596 , -117.7341596 , -124.6351596 ],\n",
       "         [-104.87838684, -117.71838684, -124.61938684],\n",
       "         ...,\n",
       "         [-105.13279306, -117.97279306, -124.87379306],\n",
       "         [-102.81730138, -115.65730138, -122.55830138],\n",
       "         [-105.22006165, -118.06006165, -124.96106165]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-105.31736289, -118.15736289, -125.05836289],\n",
       "         [-105.42424809, -118.26424809, -125.16524809],\n",
       "         [-105.51417064, -118.35417064, -125.25517064],\n",
       "         ...,\n",
       "         [-102.49455475, -115.33455475, -122.23555475],\n",
       "         [-102.47105216, -115.31105216, -122.21205216],\n",
       "         [-102.45274304, -115.29274304, -122.19374304]],\n",
       "\n",
       "        [[-105.30935561, -118.14935561, -125.05035561],\n",
       "         [-105.42093395, -118.26093395, -125.16193395],\n",
       "         [-105.50799476, -118.34799476, -125.24899476],\n",
       "         ...,\n",
       "         [-102.48616563, -115.32616563, -122.22716563],\n",
       "         [-102.52233731, -115.36233731, -122.26333731],\n",
       "         [-102.46442984, -115.30442984, -122.20542984]],\n",
       "\n",
       "        [[-105.30951499, -118.14951499, -125.05051499],\n",
       "         [-105.42514228, -118.26514228, -125.16614228],\n",
       "         [-105.49978243, -118.33978243, -125.24078243],\n",
       "         ...,\n",
       "         [-102.51586687, -115.35586687, -122.25686687],\n",
       "         [-102.51156782, -115.35156782, -122.25256782],\n",
       "         [-102.41796564, -115.25796564, -122.15896564]]],\n",
       "\n",
       "\n",
       "       [[[-102.71134948, -115.55134948, -122.45234948],\n",
       "         [-103.00172209, -115.84172209, -122.74272209],\n",
       "         [-103.09017216, -115.93017216, -122.83117216],\n",
       "         ...,\n",
       "         [-103.26804601, -116.10804601, -123.00904601],\n",
       "         [-103.33061277, -116.17061277, -123.07161277],\n",
       "         [-103.30009191, -116.14009191, -123.04109191]],\n",
       "\n",
       "        [[-102.46648692, -115.30648692, -122.20748692],\n",
       "         [-102.65994595, -115.49994595, -122.40094595],\n",
       "         [-103.07042383, -115.91042383, -122.81142383],\n",
       "         ...,\n",
       "         [-103.28225904, -116.12225904, -123.02325904],\n",
       "         [-103.39703779, -116.23703779, -123.13803779],\n",
       "         [-103.29582201, -116.13582201, -123.03682201]],\n",
       "\n",
       "        [[-102.59583973, -115.43583973, -122.33683973],\n",
       "         [-102.58928942, -115.42928942, -122.33028942],\n",
       "         [-102.59999024, -115.43999024, -122.34099024],\n",
       "         ...,\n",
       "         [-103.33683561, -116.17683561, -123.07783561],\n",
       "         [-103.35783767, -116.19783767, -123.09883767],\n",
       "         [-103.30587207, -116.14587207, -123.04687207]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-102.79909002, -115.63909002, -122.54009002],\n",
       "         [-103.01308071, -115.85308071, -122.75408071],\n",
       "         [-103.02987622, -115.86987622, -122.77087622],\n",
       "         ...,\n",
       "         [-102.88416563, -115.72416563, -122.62516563],\n",
       "         [-102.97539138, -115.81539138, -122.71639138],\n",
       "         [-102.78704571, -115.62704571, -122.52804571]],\n",
       "\n",
       "        [[-102.86968993, -115.70968993, -122.61068993],\n",
       "         [-102.99394469, -115.83394469, -122.73494469],\n",
       "         [-103.00368135, -115.84368135, -122.74468135],\n",
       "         ...,\n",
       "         [-102.90072512, -115.74072512, -122.64172512],\n",
       "         [-102.95377492, -115.79377492, -122.69477492],\n",
       "         [-102.93421947, -115.77421947, -122.67521947]],\n",
       "\n",
       "        [[-102.84789347, -115.68789347, -122.58889347],\n",
       "         [-102.88156651, -115.72156651, -122.62256651],\n",
       "         [-102.95789777, -115.79789777, -122.69889777],\n",
       "         ...,\n",
       "         [-102.9106531 , -115.7506531 , -122.6516531 ],\n",
       "         [-102.99003254, -115.83003254, -122.73103254],\n",
       "         [-102.92809736, -115.76809736, -122.66909736]]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transfer learning ##\n",
    "\n",
    "classifier_model =\"https://tfhub.dev/google/imagenet/resnet_v2_101/classification/4\"\n",
    "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "\n",
    "# classifier = Sequential([\n",
    "#     hub.KerasLayer(classifier_model, input_shape=(img_size, img_size, 1))\n",
    "# ])\n",
    "\n",
    "# feature_extractor_layer = hub.KerasLayer(\n",
    "#     feature_extractor_model, input_shape=(img_size, img_size, 1), trainable=False)\n",
    "\n",
    "# classifier.summary()\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(128, 128, 3)))\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "              \n",
    "preprocess_input(X_train_b_rgb)\n",
    "preprocess_input(X_test_b_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1024)         2098176     global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 3)            3075        dense_35[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 25,688,963\n",
      "Trainable params: 25,635,843\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Makes a bigass file for saving weights, be aware ##\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='best_weights_resnet.hdf5', save_best_only=True, save_weights_only=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[64,64,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_9/conv2_block1_1_bn/FusedBatchNormV3 (defined at <ipython-input-85-305c96e2f137>:3) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_127824]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-305c96e2f137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(X_train_b_rgb, y_train_b, steps_per_epoch=50,\n\u001b[0;32m      2\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_b_rgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m          callbacks=[checkpoint, lr_reduce, early_stop])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,64,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_9/conv2_block1_1_bn/FusedBatchNormV3 (defined at <ipython-input-85-305c96e2f137>:3) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_127824]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_b_rgb, y_train_b, steps_per_epoch=50,\n",
    "         epochs=20, batch_size=64, validation_data=(X_test_b_rgb, y_test_b),\n",
    "         callbacks=[checkpoint, lr_reduce, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 113ms/step - loss: 1.8295 - accuracy: 0.2003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.829487919807434, 0.20033389329910278]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_b_rgb, y_test_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
